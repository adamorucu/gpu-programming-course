\documentclass[a4paper,11pt]{scrartcl}

\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm} 
\usepackage{url}
\usepackage[left=20mm,top=20mm]{geometry}
\usepackage{mathpazo}
\usepackage{booktabs}
\usepackage{hyperref}


\title{Assignment III}
\author{Adam Orucu}


\begin{document}

\maketitle

Url to code: \url{https://github.com/adamorucu/gpu-programming-course}

\section*{Exercise 1}

\begin{enumerate}
    \item \textbf{Describe all optimizations you tried regardless of whether you committed to them or abandoned them and whether they improved or hurt performance. }
    \begin{itemize}
        \item I've started with a kernel that does not use shared memory.
        \item I upgraded it to use shared memory, however in this case it didn't work for large \verb|NUM_BINS|.
        \item Corrected the way I divide the shared memory and it's reseting. I had started the kernel with setting the local bins to 0, but then I changed it to be at the end in the same loop as summing up the bins.
    \end{itemize}

    \item \textbf{Which optimizations you chose in the end and why? }

    I used a local bins counter for shared memory and then add the local values togather to get the result.

    \item \textbf{How many global memory reads are being performed by your kernel? Explain }

    For the \verb|convert_kernel| 1 for each bin in the histogram. For the \verb|histogram_kernel| the length of input. Therefore $4096 + 400 = 4496$.

    \item \textbf{How many atomic operations are being performed by your kernel? Explain}
    One for every element in the input array, plus one for every thread.

    \item \textbf{How much shared memory is used in your code? Explain}

    The number of threads multiplied by the size of an unsigned integer, because that is the amount of data stored in the local bins counter.

    \item \textbf{How would the value distribution of the input array affect the contention among threads? For instance, what contentions would you expect if every element in the array has the same value? }

    They would try to write to the same location at the same time, therefore sloving the porcess.

    \item \textbf{Plot a histogram generated by your code and specify your input length, thread block and grid.}

        Input length: 50000, num\_bins: 2049, TPB: 1024, BPG: 49

\includegraphics*[width=0.8\textwidth]{images/hist.png}


    \item \textbf{For a input array of 1024 elements, profile with Nvidia Nsight and report Shared Memory Configuration Size and Achieved Occupancy. Did Nvsight report any potential performance issues?}

    I could not perform analysis, because I ran out of limit on the compute allowed in colab.

\end{enumerate}



% %%%%%%%%%%%%%
\section*{Exercise 2}

\begin{enumerate}
\item \textbf{Describe the environment you used, what changes you made to the Makefile, and how you ran the simulation.}

I ran the code on Google Colab on the Tesla T4 GPU. For the code to compile I had to change the \verb|ARCH| flag in the Makefile so that it equals \verb|sm_75|.
I ran the code with the following commands \verb|make| and \verb|./bin/sputniPIC.out inputfiles/GEM_2D.inp|.

To run:
\verb{!rm -rf /content/drive/MyDrive/DD2360/datas/
!mkdir /content/drive/MyDrive/DD2360/datas/

\%cd /content/drive/MyDrive/DD2360/sputniPIC-DD2360/
!make clean
!make
!./bin/sputniPIC.out inputfiles/GEM_2D.inp
!cp -r data /content/drive/MyDrive/DD2360/datas/cpu_data

\%cd /content/drive/MyDrive/DD2360/DD23COPY/
!make clean
!make
!./bin/sputniPIC.out inputfiles/GEM_2D.inp
!cp -r data /content/drive/MyDrive/DD2360/datas/gpu_data}

\item \textbf{Describe your design of the GPU implementation of mover\_PC() briefly. }

I added all the memory allocation, copy and memory freeing functionality in the \verb|mover_PC_gpu| function and moved the operations to a separate kernel. Within the kernel each thread makes calculations for a single particle. Some variables have been flatten in these cases the predefined \verb|get_idx| function was used to get data from them.

\item \textbf{Compare the output of both CPU and GPU implementation to guarantee that your GPU implementations produce correct answers.}

I could not compare output, because colab runtime ended before I was able to look at the results.

\item \textbf{Compare the execution time of your GPU implementation with its CPU version.}

\includegraphics*[width=0.8\textwidth]{images/01-cpurun.png}
        
\includegraphics*[width=0.8\textwidth]{images/02-gpurun.png}

Check if they are same
\verb{
!diff /content/drive/MyDrive/DD2360/datas/cpu_data/B_10.vtk /content/drive/MyDrive/DD2360/datas/gpu_data/B_10.vtk
!diff /content/drive/MyDrive/DD2360/datas/cpu_data/E_10.vtk /content/drive/MyDrive/DD2360/datas/gpu_data/E_10.vtk
!diff /content/drive/MyDrive/DD2360/datas/cpu_data/sputniPICparameters.txt /content/drive/MyDrive/DD2360/datas/gpu_data/sputniPICparameters.txt
}
\end{enumerate}


\end{document}