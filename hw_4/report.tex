\documentclass[a4paper,11pt]{scrartcl}

\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm} 
\usepackage{url}
\usepackage[left=20mm,top=20mm]{geometry}
\usepackage{mathpazo}
\usepackage{booktabs}
\usepackage{hyperref}


\title{Assignment IV}
\author{Adam Orucu}

\begin{document}

\maketitle

Url to code: \url{https://github.com/adamorucu/gpu-programming-course}

% %%%%%%%%%%%%%
\section*{Exercise 1}

\begin{enumerate}
    \item \textbf{Assume X=800 and Y=600. Assume that we decided to use a grid of 16X16 blocks. That is, each block is organized as a 2D 16X16 array of threads. How many warps will be generated during the execution of the kernel? How many warps will have control divergence? Please explain your answers.}

    There will be $ceil(800/16) * ceil(600/16) = 1900$ blocks. Since each block has $16*16=256$ threads there will be $256 * 1900 = 486,400$ threads in total. Given that each warp has 32 threads the number of warps is $486,400 / 32 = 15,200$.

    There will be extra threads because bottom edge will be out of bounds. $ceil(600/16)*16-600=8$, there will be $8*800=6400$ threads out of bounds. These however will be covered by other. blocks so there will not execute in the if statement so there will not be divergence.

    \item \textbf{Now assume X=600 and Y=800 instead, how many warps will have control divergence? Please explain your answers.}
    
    600 is not a multiple of 16, therefore both right edge will be out of bounds. $ciel(800/16) = 50$ blocks will be out of bounds which means that last block in each row will have partially filled rows which will leave them divergent. This means that $50*256/32= 400$ warps will have control divergence.

    \item \textbf{Now assume X=600 and Y=799, how many warps will have control divergence? Please explain your answers.}

    Previous calculation is repeated to get 400 warps. Additionally having 799 pixels on the other axis means that there will be an extra row, which gives extra 38 warps. However these two axes have one overlapping warp therefore the answer is $400 + 38 - 1 = 437$

\end{enumerate}



% %%%%%%%%%%%%%
\section*{Exercise 2}

\begin{enumerate}
  \item \textbf{Compared to the non-streamed vector addition, what performance gain do you get? Present in a plot ( you may include comparison at different vector length)}

  There wasn't a performance gain in fact streamed version of the code ended up being slightly slower.
    
  \includegraphics*[width=0.6\textwidth]{images/stream.png}

  \item \textbf{Use nvprof to collect traces and the NVIDIA Visual Profiler (nvvp) to visualize the overlap of communication and computation.}

  I wasn't able to make nvvp work on two different operating systems.

  \includegraphics*[width=0.6\textwidth]{images/err.png}

  \item \textbf{What is the impact of segment size on performance? Present in a plot ( you may choose a large vector and compare 4-8 different segment sizes)}

  \includegraphics*[width=0.6\textwidth]{images/segmentsize.png}

\end{enumerate}


% %%%%%%%%%%%%%
\section*{Exercise 3}


\begin{enumerate}

    \item \textbf{Run the program with different dimX values. For each one, approximate the FLOPS (floating-point operation per second) achieved in computing the SMPV (sparse matrix multiplication). Report FLOPS at different input sizes in a FLOPS. What do you see compared to the peak throughput you report in Lab2?}

    There is a linear relationship between dimX and the number of FLOPs.

    \includegraphics*[width=0.6\textwidth]{images/flops.png}

    \item \textbf{Run the program with dimX=128 and vary nsteps from 100 to 10000. Plot the relative error of the approximation at different nstep. What do you observe?}

    \includegraphics*[width=0.6\textwidth]{images/nsteps.png}

    \item \textbf{Compare the performance with and without the prefetching in Unified Memory. How is the performance impact?}

    There is a clear and significant differnce in the time for initialising the sparce matrix, wether prefetching is used.

    \includegraphics*[width=0.6\textwidth]{images/prefetch.png}

\end{enumerate}

\end{document}
